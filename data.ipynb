{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<!-- <center>\n",
    "<h2>\n",
    "MALLORN Astronomical Classification\n",
    "</h2>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "<h2></h2>\n",
    "<h3></h3>\n",
    "<h3></h3>\n",
    "<h3>\n",
    "Rasmus Eliasson & Oskar Flodin\n",
    "</h3>\n",
    "<img src=\"bth.png\" width=\"100\">\n",
    "\n",
    "<h5>\n",
    "    M.Sc. Eng. in AI & Machine Learning <br>\n",
    "    DIDA, Blekinge Institute of Technology<br>\n",
    "    Karlskrona, Sweden\n",
    "</h5>\n",
    "\n",
    "<mail>rael23@student.bth.se <br>\n",
    "osfl22@student.bth.se</mail>\n",
    "\n",
    "<p>2025-12-12</p>\n",
    "</center>\n",
    " -->\n",
    "\n",
    " <h2 style=\"text-align:center;\">MALLORN Astronomical Classification</h2>\n",
    "\n",
    "<h3 style=\"text-align:center;\">Rasmus Eliasson & Oskar Flodin</h3>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"bth.png\" width=\"100\">\n",
    "    <h5 style=\"margin:4px 0; line-height:1.2;\">\n",
    "        M.Sc. Eng. in AI & Machine Learning<br>\n",
    "        DIDA, Blekinge Institute of Technology\n",
    "    </h5>\n",
    "    <h5 style=\"margin:4px 0;\">Karlskrona, Sweden</h5>\n",
    "    <a href=\"mailto:rael23@student.bth.se\">rael23@student.bth.se</a><br>\n",
    "    <a href=\"mailto:osfl22@student.bth.se\">osfl22@student.bth.se</a>\n",
    "    <p style=\"text-align:center;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<h2>\n",
    "    Traditional Machine Learning Approach - Pre processing\n",
    "</h2>\n",
    "</center>\n"
   ],
   "id": "d0ddc385d125af28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "import extract_data as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "main_folder = \"mallorn-astronomical-classification-challenge\"\n",
    "data_folder = \"Data\"\n",
    "data_path = os.path.join(os.getcwd(), data_folder)\n",
    "re_extract_data = False\n",
    "\n",
    "path = os.path.join(os.getcwd(), main_folder)\n",
    "\n",
    "\n",
    "# data_paths_list = [os.path.join(main_folder, d) for d in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, d))]\n",
    "\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"mallorn-astronomical-classification-challenge\")\n",
    "\n",
    "split_paths = [\n",
    "    os.path.join(base_path, d)\n",
    "    for d in os.listdir(base_path)\n",
    "    if d.startswith(\"split_\") and os.path.isdir(os.path.join(base_path, d))\n",
    "]\n",
    "\n",
    "split_map = {\n",
    "    os.path.basename(p): p\n",
    "    for p in split_paths\n",
    "}\n",
    "\n"
   ],
   "id": "12818683acc12635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "def Null_check_func(split_name):\n",
    "    split_path = split_map[split_name]\n",
    "\n",
    "    split_train = pd.read_csv(\n",
    "        os.path.join(split_path, \"train_full_lightcurves.csv\")\n",
    "    )\n",
    "    split_test = pd.read_csv(\n",
    "        os.path.join(split_path, \"test_full_lightcurves.csv\")\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 8))\n",
    "\n",
    "    sns.heatmap(\n",
    "        split_train.isnull(),\n",
    "        cbar=False,\n",
    "        yticklabels=False,\n",
    "        cmap=\"viridis\",\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f\"Check for train NULLs in {split_name}\")\n",
    "\n",
    "    sns.heatmap(\n",
    "        split_test.isnull(),\n",
    "        cbar=False,\n",
    "        yticklabels=False,\n",
    "        cmap=\"viridis\",\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title(f\"Check for test NULLs in {split_name}\")\n",
    "\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    Null_check_func,\n",
    "    split_name=widgets.Dropdown(options=sorted(split_map.keys()))\n",
    ");\n"
   ],
   "id": "2c4df4dfabbba820"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "if re_extract_data:\n",
    "    df_train, df_test = ed.merge_and_save_data()\n",
    "else:\n",
    "    df_train = pd.read_csv(os.path.join(data_path, \"MALLORN-data_train.csv\"), sep=',')\n",
    "    df_test = pd.read_csv(os.path.join(data_path, \"MALLORN-data_test.csv\"), sep=',')\n",
    "\n",
    "# df_train"
   ],
   "id": "c3e469796f26144c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "exclude_cols = [\n",
    "    \"English Translation\",\n",
    "    \"object_id\",\n",
    "    \"split\",\n",
    "    \"SpecType\",\n",
    "    \"target\"\n",
    "]\n",
    "common_cols = sorted(\n",
    "    set(df_train.columns).intersection(df_test.columns) - set(exclude_cols)\n",
    ")\n",
    "\n",
    "df_train = df_train.dropna(axis=1, how=\"all\")\n",
    "df_test  = df_test.dropna(axis=1, how=\"all\")\n",
    "\n",
    "def attribute_func(Attribute):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4), sharey=True)\n",
    "\n",
    "    if Attribute in df_train.columns:\n",
    "        sns.boxplot(data=df_train[[Attribute]], ax=axes[0], color=\"darkred\")\n",
    "    axes[0].set_title(f\"Dataset Train: {Attribute}\")\n",
    "    axes[0].set_xlabel(\"\")\n",
    "\n",
    "    if (Attribute in df_test.columns and Attribute not in exclude_cols):\n",
    "        sns.boxplot(data=df_test[[Attribute]], ax=axes[1], color=\"darkred\")\n",
    "    axes[1].set_title(f\"Dataset Test: {Attribute}\")\n",
    "    axes[1].set_xlabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(attribute_func, Attribute=common_cols);"
   ],
   "id": "4d506e36644184a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# all_cols[-1]",
   "id": "b18ddf6888e3ee1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Dropdown, VBox, interactive_output\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Assumes df_train and df_test already exist\n",
    "# --------------------------------------------------\n",
    "\n",
    "df_map = {\n",
    "    \"train\": df_train,\n",
    "    \"test\": df_test\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper: create column intervals\n",
    "# --------------------------------------------------\n",
    "def make_intervals(df, group_size=6):\n",
    "    intervals = []\n",
    "    n_cols = len(df.columns)\n",
    "\n",
    "    for i in range(0, n_cols, group_size):\n",
    "        start = i + 1\n",
    "        end = min(i + group_size, n_cols)\n",
    "        intervals.append(f\"{start}-{end}\")\n",
    "\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def get_intervals(dataset):\n",
    "    return make_intervals(df_map[dataset], group_size=6)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Plot function (controlled by widgets)\n",
    "# --------------------------------------------------\n",
    "def histo_plot(dataset, interval):\n",
    "    df = df_map[dataset]\n",
    "\n",
    "    start, end = map(int, interval.split(\"-\"))\n",
    "    cols = df.columns[start - 1:end]\n",
    "\n",
    "    df[cols].hist(bins=\"auto\", figsize=(12, 8))\n",
    "    plt.suptitle(\n",
    "        f\"{dataset.upper()} histograms — columns {interval}\",\n",
    "        fontsize=18\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Widgets\n",
    "# --------------------------------------------------\n",
    "dataset_widget = Dropdown(\n",
    "    options=[\"train\", \"test\"],\n",
    "    value=\"train\",\n",
    "    description=\"Dataset:\"\n",
    ")\n",
    "\n",
    "interval_widget = Dropdown(\n",
    "    description=\"Interval:\"\n",
    ")\n",
    "\n",
    "# Update interval options when dataset changes\n",
    "def update_intervals(change):\n",
    "    interval_widget.options = get_intervals(change[\"new\"])\n",
    "    interval_widget.value = interval_widget.options[0]\n",
    "\n",
    "dataset_widget.observe(update_intervals, names=\"value\")\n",
    "update_intervals({\"new\": dataset_widget.value})  # initialize\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Layout & display\n",
    "# --------------------------------------------------\n",
    "ui = VBox([dataset_widget, interval_widget])\n",
    "\n",
    "out = interactive_output(\n",
    "    histo_plot,\n",
    "    {\n",
    "        \"dataset\": dataset_widget,\n",
    "        \"interval\": interval_widget\n",
    "    }\n",
    ")\n",
    "\n",
    "display(ui, out)\n"
   ],
   "id": "61e50e0362057572"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# print(df_train.columns.tolist())\n",
    "# # df_test"
   ],
   "id": "9d1c567796bfb22c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Assumes df_train already exists and includes `target`\n",
    "# --------------------------------------------------\n",
    "\n",
    "features = df_train.copy()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Create amplitude features (per filter)\n",
    "# --------------------------------------------------\n",
    "bands = [\"u\", \"g\", \"r\", \"i\", \"z\", \"y\"]\n",
    "\n",
    "for band in bands:\n",
    "    max_col = f\"max_flux_{band}\"\n",
    "    min_col = f\"min_flux_{band}\"\n",
    "    if max_col in features.columns and min_col in features.columns:\n",
    "        features[f\"amplitude_{band}\"] = features[max_col] - features[min_col]\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Get numeric feature list (exclude target)\n",
    "# --------------------------------------------------\n",
    "numeric_features = (\n",
    "    features\n",
    "    .select_dtypes(include=\"number\")\n",
    "    .columns\n",
    "    .drop(\"target\", errors=\"ignore\")\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Plot function (distribution + reference scatter)\n",
    "# --------------------------------------------------\n",
    "def feature_distribution_vs_class(feature, reference):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    display_name = feature.replace(\"_\", \" \").title()\n",
    "    reference_name = reference.replace(\"_\", \" \").title()\n",
    "\n",
    "    # ---- Left: distribution by class ----\n",
    "    sns.histplot(\n",
    "        data=features,\n",
    "        x=feature,\n",
    "        hue=\"target\",\n",
    "        bins=\"auto\",\n",
    "        element=\"step\",\n",
    "        kde=False,\n",
    "        ax=axes[0]\n",
    "    )\n",
    "\n",
    "    axes[0].set_title(f\"Distribution of {display_name} by Class\")\n",
    "    axes[0].set_xlabel(display_name)\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "    # ---- Right: feature vs reference ----\n",
    "    sns.scatterplot(\n",
    "        data=features,\n",
    "        x=feature,\n",
    "        y=reference,\n",
    "        hue=\"target\",\n",
    "        alpha=0.6,\n",
    "        ax=axes[1]\n",
    "    )\n",
    "\n",
    "    axes[1].set_title(f\"{display_name} vs {reference_name}\")\n",
    "    axes[1].set_xlabel(display_name)\n",
    "    axes[1].set_ylabel(reference_name)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Interactive widgets\n",
    "# --------------------------------------------------\n",
    "widgets.interact(\n",
    "    feature_distribution_vs_class,\n",
    "    feature=widgets.Dropdown(\n",
    "        options=numeric_features,\n",
    "        description=\"Feature:\"\n",
    "    ),\n",
    "    reference=widgets.Dropdown(\n",
    "        options=numeric_features,\n",
    "        description=\"Reference:\"\n",
    "    )\n",
    ");\n"
   ],
   "id": "4d2ce6bdbab9270c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Only numeric columns\n",
    "df_num = df_train.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Checks if target is boolean in teh dataframe, make it numeric\n",
    "if \"target\" in df_train.columns and \"target\" not in df_num.columns:\n",
    "    df_num[\"target\"] = pd.to_numeric(df_train[\"target\"], errors=\"coerce\")\n",
    "# Correlation on numeric values\n",
    "corr_matrix = df_num.corr(numeric_only=True)\n",
    "attributes = corr_matrix.columns.tolist()\n",
    "\n",
    "def correlation_graph(attribute = 'target'):\n",
    "    corr_with_attribute = corr_matrix[attribute].sort_values(ascending=False)\n",
    "\n",
    "    # Exclude the 'attribute' itself.\n",
    "    features = corr_with_attribute.index[corr_with_attribute.index != attribute]\n",
    "\n",
    "    plot_df = pd.DataFrame({\n",
    "        \"feature\": features,\n",
    "        \"correlation\": corr_with_attribute[features].values\n",
    "    })\n",
    "\n",
    "    # Makes some nice scale for the plot.\n",
    "    min_corr = round(plot_df['correlation'].min(), 1)\n",
    "    max_corr = round(plot_df['correlation'].max(), 1)\n",
    "    margin = 0.05\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.barplot(x=\"feature\", y=\"correlation\", data=plot_df, hue=\"correlation\", dodge=False, palette=\"coolwarm\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylim(min_corr - margin, max_corr + margin)\n",
    "    plt.title(f\"Correlation of features with {attribute}\")\n",
    "    plt.ylabel(f\"Correlation with {attribute}\")\n",
    "    # remove legend, since it does not contribute.\n",
    "    plt.legend([],[], frameon=False)\n",
    "    plt.show()\n",
    "\n",
    "widgets.interact(correlation_graph, attribute = attributes);"
   ],
   "id": "9decbc2075acf8c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#####################################\n",
    "#               PCA                 #\n",
    "#####################################\n",
    "fig, subfigs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "df_num.dropna(inplace=True)\n",
    "\n",
    "X = df_num.drop(columns=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# 3. Keep only numeric columns (VERY important for PCA / t-SNE)\n",
    "X = X.select_dtypes(include=\"number\")\n",
    "y = df_num['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# PCA computes orthogonal linear combinations of the original features.\n",
    "# PC1 captures the maximum variance direction; PC2 captures the next,\n",
    "# orthogonal direction. Class labels are not used.\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X_scaled)\n",
    "\n",
    "sns.scatterplot(x=components[:,0], y=components[:,1], hue=y, alpha=0.6, ax=subfigs[0])\n",
    "subfigs[0].set_title(\"PCA Projection of TDE (2 Components)\")\n",
    "subfigs[0].set_xlabel(\"PC1\")\n",
    "subfigs[0].set_ylabel(\"PC2\")\n",
    "\n",
    "\n",
    "#####################################\n",
    "#              t_SNE                #\n",
    "#####################################\n",
    "# t-SNE is a non-linear, unsupervised embedding that preserves local\n",
    "# neighborhood relationships. It is primarily used for visualization\n",
    "# and does not preserve global distances or class boundaries.\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=40)\n",
    "tsne_results = tsne.fit_transform(X_scaled)\n",
    "\n",
    "sns.scatterplot(x=tsne_results[:,0], y=tsne_results[:,1], hue=y, alpha=0.6, ax=subfigs[1])\n",
    "subfigs[1].set_title(\"t-SNE Projection of TDE\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#####################################\n",
    "#       Feature importance          #\n",
    "#####################################\n",
    "drop_cols = [\n",
    "    \"English Translation\",\n",
    "    \"object_id\",\n",
    "    \"split\",\n",
    "    \"target\"\n",
    "]\n",
    "\n",
    "X = df_num.drop(columns=drop_cols, errors=\"ignore\")\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# importances = rf.feature_importances_\n",
    "# indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# plt.title(\"Top 15 Most Important Features (Random Forest)\")\n",
    "# plt.barh([X.columns[i] for i in indices[:15]], importances[indices[:15]])\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.xlabel(\"Importance Score\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)  # ascending order\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.title(\"Top 15 Least important Features (Random Forest)\")\n",
    "plt.barh([X.columns[i] for i in indices[:15]], importances[indices[:15]])\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "f2c0f3aec2897b6b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<center>\n",
    "<h2>Dimensionality Reduction Analysis of TDE Feature Space</h2>\n",
    "</center>\n",
    "\n",
    "<h3>Overview</h3>\n",
    "<p>\n",
    "The figure above presents two low-dimensional projections of the TDE dataset:\n",
    "a <b>PCA projection (2 components)</b> and a <b>t-SNE projection</b>, with points\n",
    "colored by the binary target label (TDE vs. non-TDE).\n",
    "These projections are used to assess the intrinsic structure of the feature\n",
    "space and the separability of the target classes.\n",
    "</p>\n",
    "\n",
    "<h3>PCA Projection</h3>\n",
    "<p>\n",
    "The PCA projection shows that the data is highly concentrated along the first\n",
    "principal component, with substantial variance spread across a wide range of\n",
    "values. Despite this variance, there is <b>no clear linear separation</b> between\n",
    "TDE and non-TDE events. The minority TDE class appears sparsely distributed and\n",
    "largely embedded within the dominant non-TDE population.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "This suggests that the directions of maximum variance captured by PCA are not\n",
    "aligned with the decision boundary required to separate the classes, indicating\n",
    "that a simple linear model operating in the original feature space may struggle\n",
    "to discriminate between TDE and non-TDE events.\n",
    "</p>\n",
    "\n",
    "<h3>t-SNE Projection</h3>\n",
    "<p>\n",
    "The t-SNE visualization further highlights the complexity of the data structure.\n",
    "While local clustering patterns are visible, the TDE events do not form a\n",
    "distinct or isolated cluster. Instead, they are scattered throughout the\n",
    "manifold occupied by the majority class.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Because t-SNE emphasizes local neighborhood structure, the lack of a cohesive\n",
    "TDE cluster suggests that TDE events are not characterized by a single, compact\n",
    "region of feature space, but rather overlap significantly with non-TDE events\n",
    "across multiple local neighborhoods.\n",
    "</p>\n",
    "\n",
    "<h3>Implications for Model Choice</h3>\n",
    "<p>\n",
    "The strong overlap observed in both PCA and t-SNE projections implies that\n",
    "<b>logistic regression is likely to be a poor baseline model</b> for this task.\n",
    "As a linear classifier, logistic regression assumes a roughly linear decision\n",
    "boundary, which is inconsistent with the complex, overlapping structure observed\n",
    "in the projections.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "More expressive, non-linear models such as <b>tree-based ensembles</b> (e.g.,\n",
    "Random Forests, Gradient Boosting) or <b>kernel-based methods</b> are better suited\n",
    "to capture the interactions and higher-order decision boundaries implied by the\n",
    "data.\n",
    "</p>\n",
    "\n",
    "<h3>Class Imbalance Considerations</h3>\n",
    "<p>\n",
    "Another critical factor is the apparent <b>class imbalance</b>. TDE events\n",
    "represent a small fraction of the dataset, which further degrades the performance\n",
    "of linear classifiers trained with standard loss functions.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "To mitigate this, rebalancing strategies should be explored, including:\n",
    "</p>\n",
    "<ul>\n",
    "  <li>Oversampling techniques such as <b>SMOTE</b> or its variants</li>\n",
    "  <li>Class-weighted loss functions</li>\n",
    "  <li>Anomaly-detection or rare-event classification frameworks</li>\n",
    "</ul>\n",
    "\n",
    "<p>\n",
    "However, even with oversampling, the heavy overlap in feature space suggests that\n",
    "performance gains from linear models may remain limited.\n",
    "</p>\n",
    "\n",
    "<h3>Conclusion</h3>\n",
    "<p>\n",
    "Overall, the dimensionality reduction analysis indicates that TDE classification\n",
    "is a <b>non-linearly separable, imbalanced learning problem</b>. Logistic\n",
    "regression is therefore unlikely to provide strong performance without extensive\n",
    "feature engineering. The results motivate the use of non-linear models combined\n",
    "with imbalance-aware training strategies to better capture the underlying\n",
    "structure of the data.\n",
    "</p>\n"
   ],
   "id": "759d5e16a19a87af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from StackingEnsemble import StackingEnsemble\n",
    "from custom_wrapped_NN import create_nn_classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# !pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "\n",
    "random_state = 42\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state= random_state)\n",
    "\n",
    "\n",
    "\n",
    "# LogisticRegression(penalty=\"l1\", solver=\"saga\")\n",
    "# KNeighborsClassifier(n_neighbors=15, weights=\"distance\")\n",
    "\n",
    "\n",
    "# qda.fit(X_train, y_train\n",
    "\n",
    "n_negative = (y_train == 0).sum()\n",
    "n_positive = (y_train == 1).sum()\n",
    "scale_pos_weight = n_negative / n_positive\n",
    "\n",
    "\n",
    "base_models = {\n",
    "    \"nn\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"nn\", create_nn_classifier(\n",
    "            input_dim=X_train.shape[1],\n",
    "            numb_classifiers=2,\n",
    "            random_state = random_state\n",
    "        ))\n",
    "    ]),\n",
    "\n",
    "    \"nb\": GaussianNB(),\n",
    "    \n",
    "    # \"gdb\": GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, max_depth=6),\n",
    "    \"xgb\": XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        # max_depth=6,\n",
    "\n",
    "        # very important defaults\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "\n",
    "        objective=\"binary:logistic\",   # or \"multi:softprob\"\n",
    "        # objective=\"multi:softprob\",\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs = -1,\n",
    "        random_state = random_state,\n",
    "        scale_pos_weight=scale_pos_weight\n",
    "    ),\n",
    "\n",
    "    \"svm_sigmoid\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm\", SVC(kernel=\"sigmoid\", probability=True, random_state = random_state))\n",
    "    ]),\n",
    "\n",
    "    \"svm_poly\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm\", SVC(kernel=\"poly\", probability=True, random_state= random_state))\n",
    "    ]),\n",
    "\n",
    "\n",
    "    # Maybe exclude theese.\n",
    "    \"qda\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"qda\", QuadraticDiscriminantAnalysis(reg_param=0.1))\n",
    "    ]),\n",
    "\n",
    "    # # optional: add LDA instead of base logistic regression\n",
    "    # \"lda\": Pipeline([\n",
    "    #     (\"scaler\", StandardScaler()),\n",
    "    #     (\"lda\", LinearDiscriminantAnalysis())\n",
    "    # ]),\n",
    "\n",
    "    # # optional: base logistic regression (can be redundant with LR meta-model)\n",
    "    \"logreg_base\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=2000))\n",
    "    ]),\n",
    "    \"rfc\": RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "}\n",
    "\n",
    "# base_models = {\n",
    "#     \"gbdt\": XGBClassifier(...),\n",
    "#     \"svm\": SVC(probability=True, ...),\n",
    "#     \"nn\": MLPClassifier(...),\n",
    "# }\n",
    "\n",
    "# base_models[\"xgb\"]\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "meta_tree = DecisionTreeClassifier(\n",
    "    # max_depth=2,              # shallow on purpose\n",
    "    max_depth=3,              # shallow on purpose\n",
    "    min_samples_leaf=20,      # prevents tiny-leaf overfit\n",
    "    class_weight=\"balanced\",  # helps with rare TDEs\n",
    "    random_state= random_state\n",
    ")\n",
    "\n",
    "# stacked_model = StackingEnsemble(base_models, meta_model=LogisticRegression(max_iter=1000, class_weight='balanced'), n_folds=5)\n",
    "stacked_model = StackingEnsemble(base_models, meta_model=meta_tree, n_folds=5)\n",
    "\n",
    "\n"
   ],
   "id": "fee0b0624e323e96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# X_train.describe()\n",
    "\n",
    "# y_train.describe()\n",
    "\n",
    "# len(y_train)\n",
    "# y_train\n",
    "\n",
    "# base_models[\"nb\"]\n"
   ],
   "id": "1d8ffd0ddb8c179c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "if not stacked_model.check_trained:\n",
    "    stacked_model.fit(X_train, y_train)\n",
    "else:\n",
    "    stacked_model: StackingEnsemble = StackingEnsemble.load_or_create()\n",
    "\n",
    "\n"
   ],
   "id": "6f41b7005923679e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# base_models.items()\n",
    "y_pred = stacked_model.predict(x_test)\n",
    "# y_pred\n",
    "# stacked_model."
   ],
   "id": "20913269556eec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# y_pred.sum()\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# test.unique()\n",
    "\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Create display with labels from the unique classes\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=sorted(y_test.unique()))\n",
    "\n",
    "\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "id": "dff14917dd2b880e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# features = importances\n",
    "\n",
    "# popped = []\n",
    "\n",
    "# importances\n",
    "\n",
    "# indices\n",
    "\n",
    "# importances = rf.feature_importances_\n",
    "# indices = np.argsort(importances)  # ascending order\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# plt.title(\"Top 15 Least important Features (Random Forest)\")\n",
    "# plt.barh([X.columns[i] for i in indices[:15]], importances[indices[:15]])\n",
    "# plt.xlabel(\"Importance Score\")\n",
    "# plt.show()\n",
    "\n",
    "# # while len()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_current = X_train.copy()\n",
    "y_current = y_train.copy()\n",
    "x_curr_test = x_test.copy()\n",
    "y_curr_test = y_test.copy()\n",
    "\n",
    "max_drops = 20\n",
    "dropped_features = []\n",
    "\n",
    "best_score = f1_score(y_test, y_pred)\n",
    "n_dropped = 0\n",
    "\n",
    "while n_dropped < max_drops:\n",
    "\n",
    "    # Fit model\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_current, y_current)\n",
    "\n",
    "    # stacked_model.fit(X_current, y_current)\n",
    "\n",
    "    # Evaluate\n",
    "    y_pred = stacked_model.predict(x_curr_test)\n",
    "    score = f1_score(y_curr_test, y_pred)\n",
    "\n",
    "    # Stop if performance improves\n",
    "    if score > 0.5:\n",
    "        print(\"Performance above 50!\")\n",
    "        break\n",
    "    elif score > best_score:\n",
    "        print(f\"Performance improved to {score:.4f}\")\n",
    "        best_score = score\n",
    "        # break\n",
    "\n",
    "\n",
    "    # best_score = score\n",
    "\n",
    "    # Feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)  # ascending\n",
    "\n",
    "    # Least important feature\n",
    "    least_important_feature = X_current.columns[indices[0]]\n",
    "    dropped_features.append(least_important_feature)\n",
    "\n",
    "    # Drop feature\n",
    "    X_current = X_current.drop(columns=[least_important_feature])\n",
    "    x_curr_test = x_curr_test.drop(columns=[least_important_feature])\n",
    "    n_dropped += 1\n",
    "\n",
    "    stacked_model.fit(X_current, y_current)\n",
    "\n",
    "\n",
    "    print(f\"Dropped: {least_important_feature}\")\n",
    "\n",
    "print(\"Final dropped features:\", dropped_features)\n"
   ],
   "id": "4f8aad32361f7be6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "624f2703695f175a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# y_pred.sum()\n",
    "\n",
    "Stackingmodel2: StackingEnsemble = StackingEnsemble.load_or_create()\n",
    "\n",
    "y_pred = Stackingmodel2.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# test.unique()\n",
    "\n",
    "print(cm)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Create display with labels from the unique classes\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=sorted(y_test.unique()))\n",
    "\n",
    "\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "id": "135c907130b02792"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score, make_scorer, accuracy_score\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "# f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "print(f1)"
   ],
   "id": "9dd8764302e15abe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "p_val = stacked_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.6, 200)\n",
    "f1s = []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_hat = (p_val >= t).astype(int)\n",
    "    f1s.append(f1_score(y_test, y_hat, pos_label=1))\n",
    "\n",
    "best_t = thresholds[np.argmax(f1s)]\n",
    "best_f1 = max(f1s)\n",
    "\n",
    "print(\"Best threshold:\", best_t)\n",
    "print(\"Best validation F1:\", best_f1)\n",
    "\n",
    "p_test = stacked_model.predict_proba(x_test)[:, 1]\n",
    "y_test_pred = (p_test >= best_t).astype(int)\n",
    "\n"
   ],
   "id": "2e3dd9cd6dc095f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<!-- <center>\n",
    "<h3>\n",
    "This poor performance is probabbly due to:\n",
    "</h3>\n",
    "</center>\n",
    "\n",
    "<p>\n",
    "    This poor performance of the stacked model is likely due to overlapping transient features. Meaning it is hard for the base models to classify TDE:s vs non and other transients. For example a (supernova?) a supernova can have similar feature values as an TDE, but will likly be wrongly classified as `1` (TDE). This claim is also suported by how the PCA and t-SNE projections give information about the data.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    Therefore a further study and solution to this problem, would be to introduce more classifications of other `transients`. Thus likely increase the models performance on TDE:s vs Non TDE:s and other transients.\n",
    "</p> -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "    <h3>\n",
    "        <!-- Why the F1 score is relatively low -->\n",
    "        This relatively low F1 performance is likely due to strong feature overlap between TDEs and other types of astronomical transients.\n",
    "    </h3>\n",
    "</center>\n",
    "\n",
    "<p>\n",
    "  Our stacked model achieves an F1 score of roughly <strong>0.17–0.29</strong> for identifying tidal disruption events (TDEs).\n",
    "  A likely reason is <strong>limited class separability</strong>: the engineered light-curve feature vectors for TDEs overlap\n",
    "  substantially with those of non-TDE transients. In photometric feature space, several other transient types (e.g., some\n",
    "  supernova-like events) can exhibit similar rise/decay behavior and variability statistics, which increases confusion and\n",
    "  leads to <strong>many false positives</strong>.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  This interpretation is supported by the <strong>PCA</strong> and <strong>t-SNE</strong> projections, which show heavy overlap\n",
    "  between the TDE and non-TDE samples rather than cleanly separated clusters. This suggests that the performance limitation\n",
    "  is primarily driven by <strong>overlapping feature distributions</strong> (i.e., the data representation), not merely by the\n",
    "  choice of classifier or meta-model.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "  A plausible direction for future work is to use a <strong>more fine-grained labeling strategy</strong>, such as a\n",
    "  <strong>multi-class</strong> or <strong>hierarchical</strong> approach that explicitly separates major transient families\n",
    "  (e.g., supernova subtypes vs. nuclear transients) before making a final TDE decision. The goal would be to reduce\n",
    "  systematic confusion between TDEs and specific non-TDE populations, which could improve the precision–recall trade-off and\n",
    "  therefore the TDE F1 score. Note that adding more classes does not automatically guarantee improved performance; it helps\n",
    "  only if the additional labels reflect genuinely distinct and learnable structure in the data.\n",
    "</p>\n"
   ],
   "id": "2601090b9bdbe39f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8597fb15fc621f44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "model_y_pred = stacked_model.base_model_predict(x_test)\n",
    "\n",
    "\n",
    "def base_model_confusion(base_model):\n",
    "    name = 'qda'\n",
    "\n",
    "    y_pred = model_y_pred[base_model]\n",
    "\n",
    "    # from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    # test.unique()\n",
    "\n",
    "    print(cm)\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "    # Create display with labels from the unique classes\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                display_labels=sorted(y_test.unique()))\n",
    "\n",
    "\n",
    "    disp.plot(cmap=\"Blues\", values_format='d')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.show();\n",
    "\n",
    "\n",
    "widgets.interact(\n",
    "    base_model_confusion,\n",
    "    base_model=widgets.Dropdown(options=sorted(base_models.keys()))\n",
    ");\n"
   ],
   "id": "a1cc85c77b9b3038"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "190ececf71ead079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "# import numpy as np\n",
    "\n",
    "# p = stacked_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# thresholds = np.linspace(0.01, 0.5, 100)\n",
    "# f1s = [f1_score(y_test, (p >= t).astype(int)) for t in thresholds]\n",
    "\n",
    "# best_t = thresholds[np.argmax(f1s)]\n",
    "# best_f1 = max(f1s)\n",
    "\n",
    "# print(\"Best threshold:\", best_t)\n",
    "# print(\"Best F1:\", best_f1)\n"
   ],
   "id": "e4df74eded79dc5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "\n",
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "\n",
    "class_dist = pd.DataFrame({\n",
    "    \"Quality\": unique,\n",
    "    \"Count\": counts\n",
    "})\n",
    "\n",
    "print(\"Class distribution in y_pred:\")\n",
    "print(class_dist)\n"
   ],
   "id": "c7e3be35e879d823"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_prob = stacked_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "print(\n",
    "    np.min(y_prob),\n",
    "    np.percentile(y_prob, [50, 75, 90, 95, 99]),\n",
    "    np.max(y_prob)\n",
    ")\n"
   ],
   "id": "6249fa066bb10aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# “If I randomly pick one positive and one negative, how often does the model rank the positive higher?”\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC Curve (AUC = {roc_auc:.3f})\")\n",
    "plt.show()\n"
   ],
   "id": "6f82b43ef0deeb23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, p_val)\n",
    "ap = average_precision_score(y_test, p_val)\n",
    "\n",
    "print(\"PR AUC (AP):\", ap)\n"
   ],
   "id": "589f90a53501dc03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pred_dist = (\n",
    "    pd.Series(y_pred)\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .rename(\"Count_pred\")\n",
    ")\n",
    "\n",
    "\n",
    "true_dist = (\n",
    "    y\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .rename(\"Count_true\")\n",
    ")\n",
    "\n",
    "print(\"Class distribution in y_pred:\")\n",
    "print(pred_dist, \"\\n\")\n",
    "\n",
    "print(\"Class distribution in y_train:\")\n",
    "print(true_dist)\n"
   ],
   "id": "9f5fda5cb2f850a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pred_dist = (\n",
    "    pd.Series(y_pred)\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .rename(\"Count_pred\")\n",
    ")\n",
    "\n",
    "true_dist = (\n",
    "    y\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    "    .rename(\"Count_true\")\n",
    ")\n",
    "\n",
    "print(\"Class distribution in y_pred:\")\n",
    "print(pred_dist, \"\\n\")\n",
    "\n",
    "print(\"Class distribution in y_train:\")\n",
    "print(true_dist)\n",
    "\n",
    "# y.describe()\n",
    "\n",
    "\n"
   ],
   "id": "431d265c274d48d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c64f154dbaa42d15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install extinction==0.4.7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "main_folder = \"mallorn-astronomical-classification-challenge\"\n",
    "data_folder = \"Data\"\n",
    "path = os.path.join(os.getcwd(), main_folder)\n",
    "# Path to your main folder\n",
    "\n",
    "# Get all unique directories\n",
    "Data_dict = [os.path.join(main_folder, d) for d in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, d))]\n",
    "\n",
    "# print(\"Unique directories found:\")\n",
    "# for d in Data_dict:\n",
    "#     print(d)\n",
    "\n",
    "path = os.path.join(os.getcwd(), main_folder)\n",
    "path2 = os.path.join(path, 'split_01')\n",
    "# print(path)\n",
    "df_split1 = pd.read_csv(os.path.join(path2, \"train_full_lightcurves.csv\"), sep = ',')\n",
    "\n",
    "df = df_split1.copy()\n",
    "\n",
    "#Setting filter colours for later plotting\n",
    "filter_colours = {'u': '#6A5ACD', 'g': '#2ca02c', 'r': '#d62728', 'i': '#ff7f0e', 'z': '#8c564b', 'y': '#1b1b1b'}\n",
    "\n",
    "\n",
    "#Define name of chosen object\n",
    "# object_ID = 'amon_imloth_luin'\n",
    "object_ID = 'Dornhoth_fervain_onodrim'\n",
    "\n",
    "#Creating masks to isolate data for chosen lightcurve and split according to filter\n",
    "u_mask = ((df['Filter'] == 'u') & (df['object_id'] == object_ID))\n",
    "g_mask = ((df['Filter'] == 'g') & (df['object_id'] == object_ID))\n",
    "r_mask = ((df['Filter'] == 'r') & (df['object_id'] == object_ID))\n",
    "i_mask = ((df['Filter'] == 'i') & (df['object_id'] == object_ID))\n",
    "z_mask = ((df['Filter'] == 'z') & (df['object_id'] == object_ID))\n",
    "y_mask = ((df['Filter'] == 'y') & (df['object_id'] == object_ID))\n",
    "\n",
    "#Saving fluxes, times and error values for the object split according to filter\n",
    "u_flux = np.array(df.loc[u_mask, 'Flux']); u_time = np.array(df.loc[u_mask, 'Time (MJD)']); u_err = np.array(df.loc[u_mask, 'Flux_err'])\n",
    "g_flux = np.array(df.loc[g_mask, 'Flux']); g_time = np.array(df.loc[g_mask, 'Time (MJD)']); g_err = np.array(df.loc[g_mask, 'Flux_err'])\n",
    "r_flux = np.array(df.loc[r_mask, 'Flux']); r_time = np.array(df.loc[r_mask, 'Time (MJD)']); r_err = np.array(df.loc[r_mask, 'Flux_err'])\n",
    "i_flux = np.array(df.loc[i_mask, 'Flux']); i_time = np.array(df.loc[i_mask, 'Time (MJD)']); i_err = np.array(df.loc[i_mask, 'Flux_err'])\n",
    "z_flux = np.array(df.loc[z_mask, 'Flux']); z_time = np.array(df.loc[z_mask, 'Time (MJD)']); z_err = np.array(df.loc[z_mask, 'Flux_err'])\n",
    "y_flux = np.array(df.loc[y_mask, 'Flux']); y_time = np.array(df.loc[y_mask, 'Time (MJD)']); y_err = np.array(df.loc[y_mask, 'Flux_err'])"
   ],
   "id": "84195759281f45dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Loading in log data file\n",
    "# log_path = '/content/rubin_sim_data/train_log_tutorial.csv'\n",
    "log_path = 'train_log.csv'\n",
    "\n",
    "log_df = pd.read_csv(os.path.join(path, log_path))\n",
    "\n",
    "#Selecting only the data relevant to the chosen object\n",
    "object_log = log_df.loc[log_df['object_id'] == object_ID]\n",
    "print(object_log)\n",
    "\n",
    "#Defining the redshift of that object\n",
    "redshift = object_log['Z']\n",
    "\n",
    "#redshift_err = object_log['Z_err']\n",
    "#For training set, it is a spectroscopically determined redshift (with negligible error).\n",
    "#For testing set, it will be a photometrically determined redahift and will have an error value that can be loaded in using the above line.\n",
    "\n",
    "ebv = object_log['EBV']\n",
    "#Loading in the extinction coefficient value\n",
    "#Dust in the Milky Way can obscure the light from a distant object. The amount of extinction is dependent on the position of the object relative to us and the wavelength of light.\n",
    "#The extinction coefficient (EBV) value is given in the log data to simplify the de-extinction process.\n",
    "\n",
    "SpecType = object_log['SpecType']\n",
    "#For the training set, the type of the object will be listed in the 'SpecType' column\n",
    "#For the testing set, this value will not be present\n",
    "\n",
    "\n",
    "########################################################\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "\n",
    "#Importing necessary package\n",
    "from extinction import fitzpatrick99\n",
    "\n",
    "#Defining function to de-extinct a set of flux values\n",
    "def jurassic_park (flux, eff_wl):\n",
    "    A_lambda = fitzpatrick99(eff_wl, ebv * 3.1) #3.1 = Standard Milky Way value\n",
    "    flux_ext = flux * 10**((A_lambda)/2.5)\n",
    "    return flux_ext, A_lambda\n",
    "\n",
    "\n",
    "########################################################\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "#Effective wavelength for each band - sourced from SVO Filter Profile Service\n",
    "u_eff_wl = np.array([3641]); g_eff_wl = np.array([4704]); r_eff_wl = np.array([6155])\n",
    "i_eff_wl = np.array([7504]); z_eff_wl = np.array([8695]); y_eff_wl = np.array([10056])\n",
    "\n",
    "#De-extincting lightcurves\n",
    "u_flux, u_A_lambda = jurassic_park(u_flux,u_eff_wl); g_flux, g_A_lambda = jurassic_park(g_flux,g_eff_wl)\n",
    "r_flux, r_A_lambda = jurassic_park(r_flux,r_eff_wl); i_flux, i_A_lambda = jurassic_park(i_flux,i_eff_wl)\n",
    "z_flux, z_A_lumbda = jurassic_park(z_flux,z_eff_wl); y_flux, y_A_lambda = jurassic_park(y_flux,y_eff_wl)\n",
    "\n",
    "print(f'u band extinction = {u_A_lambda}'); print(f'g band extinction = {g_A_lambda}'); print(f'r band extinction = {r_A_lambda}')\n",
    "print(f'i band extinction = {i_A_lambda}'); print(f'z band extinction = {z_A_lumbda}'); print(f'y band extinction = {y_A_lambda}')\n",
    "\n",
    "########################################################\n",
    "########################################################\n",
    "########################################################\n",
    "\n",
    "plt.figure(figsize = [10,6])\n",
    "plt.errorbar(u_time,u_flux, yerr= u_err,label='u', fmt = '.',color = filter_colours['u'],zorder=4)\n",
    "plt.errorbar(g_time,g_flux,yerr=g_err, label='g', fmt = '.', color = filter_colours['g'],zorder=5)\n",
    "plt.errorbar(r_time,r_flux,yerr = r_err, label='r', fmt = '.', color = filter_colours['r'],zorder=6)\n",
    "plt.errorbar(i_time,i_flux,yerr = i_err, label='i', fmt = '.', color = filter_colours['i'],zorder=3)\n",
    "plt.errorbar(z_time,z_flux,yerr = z_err, label='z', fmt = '.', color = filter_colours['z'],zorder=2)\n",
    "plt.errorbar(y_time,y_flux,yerr = y_err, label='y', fmt = '.', color = filter_colours['y'],zorder=1)\n",
    "plt.xlabel('Days (MJD)')\n",
    "plt.ylabel('Flux (μJy)')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5f6707a8a7f300cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "181898110962f90a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d924fe00be908f2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2ac07c12822fe049"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data = pd.merge()\n",
    "data = df.merge(log_df, on=\"object_id\") #, drop = 'English Translation\t')\n",
    "\n",
    "# log_df\n",
    "# df_split1\n",
    "# data.head(10)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,10));\n",
    "data.hist(bins='auto', figsize=(12,8));\n",
    "plt.suptitle(\"Mallorn - Histograms for All Attributes\", fontsize=20);\n",
    "plt.tight_layout();\n",
    "plt.show();\n"
   ],
   "id": "1d438730c3572915"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(data.isnull(), cbar=False, yticklabels=False, cmap='viridis');\n",
    "plt.title(\"Check for white wine NULLS\")\n",
    "plt.show();"
   ],
   "id": "bedb1ceeb2385a65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "# attributes = data[columns]\n",
    "exclude = ['object_id']\n",
    "attributes = [c for c in data.columns if c not in exclude]\n",
    "\n",
    "\n",
    "def attribute_func(Atribute):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.boxplot(\n",
    "        color=\"darkred\",\n",
    "        data = data[Atribute]\n",
    "    )\n",
    "    plt.title(f\"{Atribute}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "widgets.interact(attribute_func, Atribute = attributes);"
   ],
   "id": "bd198ca451fcf958"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Correlation check #\n",
    "\n",
    "# data.drop(columns=['Z_err', 'English Translation', 'split'])\n",
    "numeric_data = data.select_dtypes(include=['number'])\n",
    "numeric_data.drop(inplace=True, columns='Z_err')\n",
    "plt.figure(figsize=(14,10))\n",
    "sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title(\"correlation heatmap\", fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(14,10))\n",
    "# sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
    "# plt.xticks(rotation=40)\n",
    "# plt.title(\"Red wine correlation heatmap\", fontsize = 18)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "id": "c8ce251ea8d8bba2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.target.value_counts(normalize=True)",
   "id": "68bb2653e7636eb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.columns",
   "id": "b0b2148e0a8a4ceb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "tests:",
   "id": "9ac6bdbf69d1620e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "friedman_stat , friedman_p = stacked_model.friedman\n",
    "print(friedman_stat,friedman_p)"
   ],
   "id": "9dcb67a9cbd1fa6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(stacked_model.posthoc_nemenyi)",
   "id": "75bfdd412bb7dbef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
